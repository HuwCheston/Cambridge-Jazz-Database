{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pianist classification using melodic and rhythmic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import dependencies and set constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src import utils\n",
    "from src.detect.midi_utils import *\n",
    "from src.features.melody_features import *\n",
    "from src.features.rhythm_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.parallel_backend = 'threading'\n",
    "print(joblib.parallel_backend)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smallest_n_gram = 3\n",
    "largest_n_gram = 10\n",
    "MAX_INTERVAL = 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get filepaths for processing\n",
    "root = f'{utils.get_project_root()}/data/cambridge-jazz-trio-database-v01/corpus_chronology'\n",
    "files = [os.path.join(root, f) for f in os.listdir(root)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load in onsets and beats for each track\n",
    "oms = [utils.load_track_from_files(f) for f in files]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract melody for each track\n",
    "mms = [MelodyMaker(os.path.join(fp, 'piano_midi.mid'), om) for fp, om in zip(files, oms)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create histogram of pitch classes for one track"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a histogram for the pitch classes in one track\n",
    "# Extract the melody for our first track\n",
    "mel = list(mms[201].extract_melody())\n",
    "# Count the pitch classes\n",
    "pitch_classes = Counter([m.note for m in mel])\n",
    "# Sort the pitch classes in order of the piano keys\n",
    "sorted_pitches = {i: pitch_classes[i] for i in utils.ALL_PITCHES}\n",
    "# Create the bar chart\n",
    "plt.bar(sorted_pitches.keys(), sorted_pitches.values())\n",
    "plt.title(oms[201].item['track_name'] + ' — ' + oms[200].item['pianist'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gram extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract n-grams which appear in more than 3 solos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = {}\n",
    "for melody, onsets in tqdm(zip(mms, oms)):\n",
    "    mel = list(melody.extract_melody())\n",
    "    sequence = list(melody.extract_intervals(mel))\n",
    "    if len(sequence) < 2:\n",
    "        continue\n",
    "    for start in range(len(sequence)):\n",
    "        for n in range(smallest_n_gram, largest_n_gram + 1):\n",
    "            n_gram = tuple([i.interval for i in sequence[start: start + n]])\n",
    "            if any(abs(interval) > MAX_INTERVAL for interval in n_gram):\n",
    "                continue\n",
    "            if len(n_gram) == n:\n",
    "                if str(n_gram) not in res.keys():\n",
    "                    res[str(n_gram)] = {\n",
    "                        'tracks': set(),\n",
    "                        'pianists': set(),\n",
    "                        'uses': 0,\n",
    "                        'n': len(n_gram)\n",
    "                    }\n",
    "                res[str(n_gram)]['tracks'].add(onsets.item['mbz_id'])\n",
    "                res[str(n_gram)]['pianists'].add(onsets.item['pianist'])\n",
    "                res[str(n_gram)]['uses'] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_ngrams = set(ng for ng, vals in res.items() if len(vals['tracks']) >= 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract only valid n-grams from entire solos (no chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "valid_dict = {ng: 0 for ng in valid_ngrams}\n",
    "for melody, onsets in tqdm(zip(mms, oms)):\n",
    "    mel = list(melody.extract_melody())\n",
    "    sequence = list(melody.extract_intervals(mel))\n",
    "    track_results = {'pianist': onsets.item['pianist'], 'track_name': onsets.item['track_name'], 'mbz_id': onsets.item['mbz_id'], **valid_dict}\n",
    "    for start in range(len(sequence)):\n",
    "        for n in range(smallest_n_gram, largest_n_gram + 1):\n",
    "            n_gram = tuple([i.interval for i in sequence[start: start + n]])\n",
    "            if len(n_gram) == n:\n",
    "                n_gram = str(n_gram)\n",
    "                if n_gram not in valid_ngrams:\n",
    "                    continue\n",
    "                track_results[n_gram] += 1\n",
    "    ngrams.append(track_results)\n",
    "\n",
    "    \n",
    "    # using chunks\n",
    "    # for chunk in melody.chunk_melody(sequence, overlapping_chunks=False, chunk_measures=4):\n",
    "    #     if len(chunk) < 2:\n",
    "    #         continue\n",
    "    #     track_results = {'pianist': onsets.item['pianist'], 'track_name': onsets.item['track_name'], **valid_dict}\n",
    "    #     for start in range(len(chunk)):\n",
    "    #         for n in range(smallest_n_gram, largest_n_gram + 1):\n",
    "    #             n_gram = tuple([i.interval for i in chunk[start: start + n]])\n",
    "    #             if len(n_gram) == n:\n",
    "    #                 n_gram = str(n_gram)\n",
    "    #                 if n_gram not in valid_ngrams:\n",
    "    #                     continue\n",
    "    #                 track_results[n_gram] += 1\n",
    "    #     ngrams.append(track_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To ensure repeatable outputs, we need to ensure that our samples and features always follow the same order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tracks = pd.DataFrame.from_dict(ngrams).sort_values(by=['pianist', 'track_name']).set_index(['pianist', 'track_name', 'mbz_id'])\n",
    "# tracks = pd.read_csv('track_results.csv', index_col=0).sort_values(by=['pianist', 'track_name']).set_index(['pianist', 'track_name', 'mbz_id'])\n",
    "tracks = tracks.reindex(sorted(tracks.columns), axis=1).reset_index(drop=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tracks.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tracks.iloc[:5, :5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model fitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tree-based methods\n",
    "- Random Forest ✅\n",
    "- Gradient-Boosted Trees ✅\n",
    "\n",
    "Non-Tree based ML:\n",
    "- SVM ✅\n",
    "- Naive Bayes\n",
    "- Multiclass logistic regression\n",
    "\n",
    "DL\n",
    "- Perceptron (ANN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into predictors and response\n",
    "X = tracks.iloc[:, 3:].to_numpy()\n",
    "y = tracks.iloc[:, 0].to_numpy()\n",
    "# Express n-gram numbers as percentage of total track\n",
    "X = np.true_divide(X, X.sum(axis=1, keepdims=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initial_fit(estimator):\n",
    "    accs = []\n",
    "    for train_idx, test_idx in tqdm(cv.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_predict = estimator.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_predict)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(random_state=42, n_splits=5, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1, verbose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing using default parameters\n",
    "rf_acc = initial_fit(rf)\n",
    "print(rf_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 58% using only n-grams contained in at least 3 solos\n",
    "- 50% using all n-grams!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient boosted trees"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42, verbose=0, max_features='sqrt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing using default parameters\n",
    "gbc_acc = initial_fit(gbc)\n",
    "print(gbc_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svc = SVC(random_state=42, decision_function_shape='ovo', verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Need to scale the data for svms (i.e. Z-score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scale = StandardScaler().fit_transform(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing using default parameters\n",
    "svc_acc = initial_fit(svc)\n",
    "print(svc_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Without tuning, RF outperforms gradient boosted trees and SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial hyperparameter optimisation\n",
    "Find some sensible values to use prior to RF feature selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# These are the parameters we'll sample from when optimizing\n",
    "test_params = dict(\n",
    "    # the loss function to use when splitting a node\n",
    "    criterion=['gini', 'entropy', 'log_loss'],\n",
    "    # The number of trees to grow in the forest\n",
    "    n_estimators=[i for i in range(10, 200, 10)],\n",
    "    # Max number of features considered for splitting a node\n",
    "    max_features=[None, 'sqrt', 'log2'],\n",
    "    # Max number of levels in each tree\n",
    "    max_depth=[None, *[i for i in range(1, 51, 10)]],\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split=[i for i in range(2, 11)],\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf=[i for i in range(1, 11)],\n",
    "    # Whether to sample data points with our without replacement\n",
    "    bootstrap=[True, False],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1, verbose=False)\n",
    "rs = RandomizedSearchCV(rf, param_distributions=test_params, cv=cv, random_state=42, n_iter=1000, verbose=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# {'n_estimators': 170,\n",
    " # 'min_samples_split': 10,\n",
    " # 'min_samples_leaf': 2,\n",
    " # 'max_features': 'sqrt',\n",
    " # 'max_depth': 21,\n",
    " # 'criterion': 'gini',\n",
    " # 'bootstrap': False}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hard-coded results from running the above\n",
    "initial_params = {'n_estimators': 190,\n",
    "                  'min_samples_split': 8,\n",
    "                  'min_samples_leaf': 3,\n",
    "                  'max_features': 'sqrt',\n",
    "                  'max_depth': 41,\n",
    "                  'criterion': 'gini',\n",
    "                  'bootstrap': False}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature selection from random forest\n",
    "Considering only the *n*th percentile of most important features "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_initial = RandomForestClassifier(random_state=42, n_jobs=-1, verbose=False, **initial_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_importances(X_, y_, est = None) -> float:\n",
    "    if est is None:\n",
    "        est = rf_initial\n",
    "    imports = []\n",
    "    for train_idx, test_idx in cv.split(X_, y_):\n",
    "        X_train, y_train = X_[train_idx], y_[train_idx]\n",
    "        X_test, y_test = X_[test_idx], y_[test_idx]\n",
    "        rf_initial.fit(X_train, y_train)\n",
    "        imports.append(rf_initial.feature_importances_)\n",
    "    return imports\n",
    "\n",
    "def get_most_important_feature_idxs(imports, n: int = 95):\n",
    "    means = np.mean(np.vstack(imports), axis=0)\n",
    "    order = means.argsort()\n",
    "    ranks = order.argsort()\n",
    "    perc = (len(ranks) / 100) * n\n",
    "    to_keep = np.argwhere(ranks > perc)[:, 0]\n",
    "    return to_keep\n",
    "\n",
    "def get_accuracy(X_d, y_):\n",
    "    accs = []\n",
    "    for train_idx, test_idx in cv.split(X_d, y_):\n",
    "        X_train, y_train = X_d[train_idx], y_[train_idx]\n",
    "        X_test, y_test = X_d[test_idx], y_[test_idx]\n",
    "        rf_initial.fit(X_train, y_train)\n",
    "        y_predict = rf_initial.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_predict)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)\n",
    "\n",
    "\n",
    "measures = []\n",
    "imports = get_importances(X, y)\n",
    "for n in tqdm(range(0, 95, 1)):\n",
    "    n_imports = get_most_important_feature_idxs(imports, n)\n",
    "    measures.append((100 - n, get_accuracy(X[:, n_imports], y)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "measures_df = pd.DataFrame(measures)\n",
    "plt.plot(measures_df[0], measures_df[1])\n",
    "plt.ylabel('Mean accuracy across all folds ($k$=5)')\n",
    "plt.xlabel('% of all features considered')\n",
    "plt.title('Values show smoothed averages (window=10%)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perc_features_to_keep = measures_df.iloc[measures_df[1].sort_values(ascending=False).index[1], 0]\n",
    "# perc_features_to_keep = 34\n",
    "n_imports = get_most_important_feature_idxs(imports, 100 - perc_features_to_keep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_min_features = X[:, n_imports]\n",
    "print(get_accuracy(X_min_features, y), perc_features_to_keep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only need to include the ~33% most important features to obtain similar accuracy when compared to including all predictors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final hyperparameter optimisation\n",
    "\n",
    "**TODO**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot confusion matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = []\n",
    "acts = []\n",
    "ids = []\n",
    "for train_idx, test_idx in tqdm(cv.split(X_min_features, y)):\n",
    "    X_train, y_train = X_min_features[train_idx], y[train_idx]\n",
    "    X_test, y_test = X_min_features[test_idx], y[test_idx]\n",
    "    rf_initial.fit(X_train, y_train)\n",
    "    y_predict = rf_initial.predict(X_test)\n",
    "    preds.extend(y_predict)\n",
    "    acts.extend(y_test)\n",
    "    ids.extend(tracks.iloc[test_idx, 2].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_df = (\n",
    "    pd.concat([pd.Series(ids), pd.Series(acts), pd.Series(preds)], axis=1)\n",
    "    .rename(columns={0: 'mbz_id', 1: 'actual', 2: 'predicted'})\n",
    "    .apply(lambda x: [i.split(' ')[-1] for i in x])\n",
    "    .sort_values(by='actual')\n",
    "    .reset_index(drop=True)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "labs = pred_df['actual'].unique()\n",
    "cm = confusion_matrix(pred_df['actual'], pred_df['predicted'], normalize='true')\n",
    "g = sns.heatmap(cm, cmap='Reds', annot=True, fmt='.2f')\n",
    "g.set_xticks(g.get_xticks(), labels=labs, rotation=90)\n",
    "g.set_yticks(g.get_yticks(), labels=labs, rotation=00)\n",
    "g.set(xlabel='Predicted', ylabel='Actual')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Most indicative n-gram for each particular pianist\n",
    "- Fit binary classifier - i.e. is it John Hicks vs someone else? - and extract feature importances\n",
    "    - How to get direction (i.e. this n-gram definitely makes it Hicks, or means its definitely not Hicks?)\n",
    "    - Possibly -- extract *n* most important n-grams from binary random forest, fit logistic regression using these, check log odds?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pianist_labels, y_int = np.unique(y, return_inverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# We don't care that the model won't converge now and will bruteforce this later by increasing max_iter\n",
    "warnings.simplefilter('ignore', ConvergenceWarning, )\n",
    "\n",
    "fs = []\n",
    "accs = []\n",
    "c_range = range(10, 100)\n",
    "\n",
    "\n",
    "def predict(c_):\n",
    "    lr = LogisticRegression(C=c_, penalty='l2', random_state=42, solver='lbfgs', multi_class='ovr', n_jobs=1, max_iter=100)\n",
    "    accs = []\n",
    "    fs = []\n",
    "    for train_idx, test_idx in cv.split(X_min_features, y_int):\n",
    "        X_train, y_train = X_min_features[train_idx], y_int[train_idx]\n",
    "        X_test, y_test = X_min_features[test_idx], y_int[test_idx]\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        f = f1_score(y_test, yhat, average='macro')\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        fs.append(f)\n",
    "        accs.append(acc)\n",
    "    return c_, np.mean(f), np.mean(acc)\n",
    "\n",
    "with Parallel(n_jobs=-1, verbose=5) as par:\n",
    "    cv_res = par(delayed(predict)(c) for c in range(5, 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_c = 58\n",
    "print(predict(c_=best_c))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = tracks.columns[3:]\n",
    "for p in range(10):\n",
    "    pianist = pianist_labels[p]\n",
    "    best = cols[n_imports[np.argmax(np.square(lr.coef_[p, :]))]]\n",
    "    worst = cols[n_imports[np.argmin(np.square(lr.coef_[p, :]))]]\n",
    "    print(pianist, best, worst)\n",
    "    print(tracks[tracks['pianist'] == pianist][best].sum())\n",
    "    print(tracks[best].sum())\n",
    "    print(tracks[tracks['pianist'] == pianist][worst].sum())\n",
    "    print(tracks[worst].sum())\n",
    "    # print(X_min_features[np.argmin(np.square(lr.coef_[p, :]))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=best_c, penalty='l2', random_state=42, solver='lbfgs', multi_class='multinomial', n_jobs=1, max_iter=100)\n",
    "lr.fit(X_min_features, y_int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Integrating rhythmic features\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rhythm = pd.read_csv(f'{utils.get_project_root()}/notebooks/prediction/rhythm_features.csv', index_col=0).reset_index(drop=True)\n",
    "rhythm['mbz_id'].dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_sort = np.hstack((\n",
    "    tracks.sort_values('mbz_id').reset_index(drop=True).iloc[:, 3:].to_numpy()[:, n_imports],\n",
    "    rhythm.iloc[:, 3:].to_numpy()\n",
    "))\n",
    "y_sort = tracks.sort_values('mbz_id').reset_index(drop=True).iloc[:, 0].to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imports = get_importances(X_sort, y_sort)\n",
    "measures = []\n",
    "for n in tqdm(range(0, 95, 1)):\n",
    "    n_imports = get_most_important_feature_idxs(imports, n)\n",
    "    ac = get_accuracy(X_sort[:, n_imports], y_sort)\n",
    "    print(n, ac)\n",
    "    measures.append((100 - n, ac))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/__init__.py\", line 113, in <module>\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/__init__.py\", line 120, in <module>\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/__init__.py\", line 113, in <module>\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/__init__.py\", line 113, in <module>\n",
      "    from .memory import Memory, MemorizedResult, register_store_backend\n",
      "    from .memory import Memory, MemorizedResult, register_store_backend\n",
      "    from .parallel import Parallel\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/memory.py\", line 32, in <module>\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/memory.py\", line 32, in <module>\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/parallel.py\", line 26, in <module>\n",
      "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 17, in <module>\n",
      "    from .pool import MemmappingPool\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/pool.py\", line 31, in <module>\n",
      "    from ._store_backends import StoreBackendBase, FileSystemStoreBackend\n",
      "    from ._store_backends import StoreBackendBase, FileSystemStoreBackend\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/_store_backends.py\", line 15, in <module>\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/_store_backends.py\", line 15, in <module>\n",
      "    from ._memmapping_reducer import get_memmapping_reducers\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/_memmapping_reducer.py\", line 37, in <module>\n",
      "    from .backports import concurrency_safe_rename\n",
      "    from .externals.loky.backend import resource_tracker\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/backports.py\", line 22, in <module>\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/externals/loky/__init__.py\", line 20, in <module>\n",
      "    from .backports import concurrency_safe_rename\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/backports.py\", line 125, in <module>\n",
      "    import distutils  # noqa\n",
      "    import numpy as np\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/numpy/__init__.py\", line 144, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
      "    from .reusable_executor import get_reusable_executor\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/externals/loky/reusable_executor.py\", line 11, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/_distutils_hack/__init__.py\", line 97, in find_spec\n",
      "    from . import core\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/numpy/core/__init__.py\", line 103, in <module>\n",
      "    return method()\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/_distutils_hack/__init__.py\", line 108, in spec_for_distutils\n",
      "    mod = importlib.import_module('setuptools._distutils')\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    from . import _internal\n",
      "  File \"/home/hwc31/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/numpy/core/_internal.py\", line 146, in <module>\n",
      "    format_re = re.compile(r'(?P<order1>[<>|=]?)'\n",
      "  File \"/usr/lib/python3.10/re.py\", line 251, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.10/re.py\", line 303, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.10/sre_compile.py\", line 788, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 955, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 444, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 846, in _parse\n",
      "    state.closegroup(group, p)\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 98, in closegroup\n",
      "    self.groupwidths[gid] = p.getwidth()\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 199, in getwidth\n",
      "    i, j = av[2].getwidth()\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 199, in getwidth\n",
      "    i, j = av[2].getwidth()\n",
      "  File \"/usr/lib/python3.10/sre_parse.py\", line 198, in getwidth\n",
      "    elif op in _REPEATCODES:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[173], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m c_, np\u001B[38;5;241m.\u001B[39mmean(f), np\u001B[38;5;241m.\u001B[39mmean(acc)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m par:\n\u001B[0;32m---> 29\u001B[0m     cv_res \u001B[38;5;241m=\u001B[39m \u001B[43mpar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1098\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[0;32m~/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[0;32m~/.virtualenvs/Cambridge-Jazz-Trio-Database/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m--> 453\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# We don't care that the model won't converge now and will bruteforce this later by increasing max_iter\n",
    "warnings.simplefilter('ignore', ConvergenceWarning, )\n",
    "\n",
    "fs = []\n",
    "accs = []\n",
    "c_range = range(10, 100)\n",
    "\n",
    "\n",
    "def predict(c_):\n",
    "    lr = LogisticRegression(C=c_, penalty='l2', random_state=42, solver='lbfgs', multi_class='ovr', n_jobs=1, max_iter=100)\n",
    "    accs = []\n",
    "    fs = []\n",
    "    for train_idx, test_idx in cv.split(X_min_features, y_int):\n",
    "        X_train, y_train = X_min_features[train_idx], y_int[train_idx]\n",
    "        X_test, y_test = X_min_features[test_idx], y_int[test_idx]\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        f = f1_score(y_test, yhat, average='macro')\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        fs.append(f)\n",
    "        accs.append(acc)\n",
    "    return c_, np.mean(f), np.mean(acc)\n",
    "\n",
    "with Parallel(n_jobs=-1, verbose=5) as par:\n",
    "    cv_res = par(delayed(predict)(c) for c in range(5, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 0.6666883116883117, 0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "best_c = 58\n",
    "print(predict(c_=best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmad Jamal (-1, -1, -1) (3, 2, 3, -1, -2)\n",
      "162\n",
      "925\n",
      "1\n",
      "6\n",
      "Bill Evans (1, 1, 1) (-3, 8, -1)\n",
      "230\n",
      "1451\n",
      "1\n",
      "19\n",
      "Bud Powell (1, 1, 1) (4, 3, -3)\n",
      "286\n",
      "1451\n",
      "12\n",
      "152\n",
      "John Hicks (1, 1, 1) (-13, -2, -2)\n",
      "253\n",
      "1451\n",
      "1\n",
      "8\n",
      "Junior Mance (12, -12, 12) (1, 2, 4, -3)\n",
      "231\n",
      "364\n",
      "1\n",
      "8\n",
      "Keith Jarrett (1, 1, 1) (-5, 14, -1)\n",
      "136\n",
      "1451\n",
      "1\n",
      "3\n",
      "Kenny Barron (0, 0, 0) (-9, 5, 4, 1)\n",
      "42\n",
      "637\n",
      "1\n",
      "3\n",
      "McCoy Tyner (1, 1, 1) (-17, -3, 3)\n",
      "5\n",
      "1451\n",
      "1\n",
      "13\n",
      "Oscar Peterson (1, 1, 1) (-7, 4, -7)\n",
      "20\n",
      "1451\n",
      "1\n",
      "12\n",
      "Tommy Flanagan (-2, -1, -2) (-6, -2, 7)\n",
      "152\n",
      "841\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "cols = tracks.columns[3:]\n",
    "for p in range(10):\n",
    "    pianist = pianist_labels[p]\n",
    "    best = cols[n_imports[np.argmax(np.square(lr.coef_[p, :]))]]\n",
    "    worst = cols[n_imports[np.argmin(np.square(lr.coef_[p, :]))]]\n",
    "    print(pianist, best, worst)\n",
    "    print(tracks[tracks['pianist'] == pianist][best].sum())\n",
    "    print(tracks[best].sum())\n",
    "    print(tracks[tracks['pianist'] == pianist][worst].sum())\n",
    "    print(tracks[worst].sum())\n",
    "    # print(X_min_features[np.argmin(np.square(lr.coef_[p, :]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=58, multi_class=&#x27;multinomial&#x27;, n_jobs=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=58, multi_class=&#x27;multinomial&#x27;, n_jobs=1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=58, multi_class='multinomial', n_jobs=1, random_state=42)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=best_c, penalty='l2', random_state=42, solver='lbfgs', multi_class='multinomial', n_jobs=1, max_iter=100)\n",
    "lr.fit(X_min_features, y_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Integrating rhythmic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhythm = pd.read_csv(f'{utils.get_project_root()}/notebooks/prediction/rhythm_features.csv', index_col=0).reset_index(drop=True)\n",
    "rhythm['mbz_id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sort = np.hstack((\n",
    "    tracks.sort_values('mbz_id').reset_index(drop=True).iloc[:, 3:].to_numpy()[:, n_imports],\n",
    "    rhythm.iloc[:, 3:].to_numpy()\n",
    "))\n",
    "y_sort = tracks.sort_values('mbz_id').reset_index(drop=True).iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports = get_importances(X_sort, y_sort)\n",
    "measures = []\n",
    "for n in tqdm(range(0, 95, 1)):\n",
    "    n_imports = get_most_important_feature_idxs(imports, n)\n",
    "    ac = get_accuracy(X_sort[:, n_imports], y_sort)\n",
    "    print(n, ac)\n",
    "    measures.append((100 - n, ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}