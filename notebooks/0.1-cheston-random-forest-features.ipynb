{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random forest for pianist classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies, set constants etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from joblib import Parallel, delayed, dump\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.inspection._permutation_importance import _weights_scorer, _create_importances_bunch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterSampler, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import src.visualise.visualise_utils as vutils\n",
    "from src import utils\n",
    "from src.detect.detect_utils import OnsetMaker\n",
    "from src.features.features_utils import *\n",
    "from src.visualise.random_forest_plots import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# These variables are used for the optimization process\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "# Number of iterations to use in random sampling\n",
    "N_ITER = 10000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the seed in NumPy for consistent results across function calls\n",
    "np.random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define predictor and prediction variables, get mappings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define target/prediction variables\n",
    "EXOG_INS = 'piano'\n",
    "PREDICTION = 'pianist'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# These are the underlying categories each predictor belongs to\n",
    "PREDICTORS_CATEGORIES = {\n",
    "    'Swing': ['bur_log_mean', 'bur_log_std'],\n",
    "    'Complexity': ['lz77_mean', 'lz77_std', 'n_onsets_mean', 'n_onsets_std'],\n",
    "    'Feel': ['bass_prop_async_nanmean', 'drums_prop_async_nanmean', 'bass_prop_async_nanstd', 'drums_prop_async_nanstd'],\n",
    "    'Interaction': ['self_coupling', 'coupling_drums', 'coupling_bass', 'coupling_piano_drums', 'coupling_piano_bass'],\n",
    "    'Tempo': ['rolling_std_median', 'tempo', 'tempo_slope',]\n",
    "}\n",
    "PREDICTORS = [it for sl in list(PREDICTORS_CATEGORIES.values()) for it in sl]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This creates a dictionary for mapping a predictor onto its category\n",
    "CATEGORY_MAPPING = {}\n",
    "for pred in PREDICTORS:\n",
    "    for category in PREDICTORS_CATEGORIES.keys():\n",
    "        if pred in PREDICTORS_CATEGORIES[category]:\n",
    "            CATEGORY_MAPPING[pred] = category"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load in data\n",
    "\n",
    "First, we load in our list of `src.detect.detect_utils.OnsetMaker` classes. These contain the location of detected onsets and beats, as well as additional metadata."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onsets: list[OnsetMaker] = utils.unserialise_object(fr'{utils.get_project_root()}\\models\\matched_onsets_corpus_chronology')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract features\n",
    "\n",
    "Now, we can extract our desired feature from each OnsetMaker class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_feature_data(feature_cls, cols, extra_str = '', **cls_kwargs):\n",
    "    \"\"\"Creates a class with given kwargs and returns the desired key-value pairs from its summary dictionary\"\"\"\n",
    "    cls = feature_cls(**cls_kwargs)\n",
    "    return {k + extra_str: v for k, v in cls.summary_dict.items() if k in cols}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_track(track: OnsetMaker) -> dict:\n",
    "    \"\"\"Processes a single track, extracting all required features, and returns a dictionary\"\"\"\n",
    "    # Convert the summary dictionary (dictionary of arrays) to a dataframe\n",
    "    summary_dict = pd.DataFrame(track.summary_dict)\n",
    "    # These are the positions of downbeats, i.e. the first beat of a measure\n",
    "    downbeats = track.ons['downbeats_manual']\n",
    "    # The tempo and time signature of the track\n",
    "    tempo = track.tempo\n",
    "    time_signature = track.item['time_signature']\n",
    "    # Subset to get my onsets and partner onsets as separate dataframes\n",
    "    my_onsets = track.ons[EXOG_INS]\n",
    "    my_beats = summary_dict[EXOG_INS]\n",
    "    their_beats = summary_dict[[i for i in utils.INSTRUMENTS_TO_PERFORMER_ROLES.keys() if i != EXOG_INS]]\n",
    "    # BEAT-UPBEAT RATIO\n",
    "    bur = get_feature_data(\n",
    "        BeatUpbeatRatio, ['bur_log_mean', 'bur_log_std', 'bur_log_count_nonzero'],\n",
    "        my_onsets=my_onsets, my_beats=my_beats, clean_outliers=True\n",
    "    )\n",
    "    # PHASE CORRECTION\n",
    "    pc = get_feature_data(\n",
    "        PhaseCorrection, ['self_coupling', 'coupling_bass', 'coupling_drums', 'nobs'],\n",
    "        my_beats=my_beats, their_beats=their_beats, order=1\n",
    "    )\n",
    "    # PHASE CORRECTION - PARTNER\n",
    "    # In comparison to the 'full' phase correction model, we only need to get a few columns here\n",
    "    pcb = get_feature_data(\n",
    "        PhaseCorrection, ['coupling_piano', 'nobs'], extra_str='_bass',\n",
    "        my_beats=summary_dict['bass'], their_beats=summary_dict[['piano', 'drums']], order=1\n",
    "    )\n",
    "    pcd = get_feature_data(\n",
    "        PhaseCorrection, ['coupling_piano', 'nobs'], extra_str='_drums',\n",
    "        my_beats=summary_dict['drums'], their_beats=summary_dict[['piano', 'bass']], order=1\n",
    "    )\n",
    "    # PROPORTIONAL ASYNCHRONY\n",
    "    pa = get_feature_data(\n",
    "        ProportionalAsynchrony, ['prop_async_count_nonzero', 'bass_prop_async_nanmean', 'drums_prop_async_nanmean', 'bass_prop_async_nanstd', 'drums_prop_async_nanstd'],\n",
    "        summary_df=summary_dict, my_instr_name=EXOG_INS\n",
    "    )\n",
    "    # IOI COMPLEXITY\n",
    "    ioi = get_feature_data(\n",
    "        IOIComplexity, ['lz77_mean', 'lz77_std', 'n_onsets_mean', 'n_onsets_std', 'window_count', 'ioi_count'],\n",
    "        my_onsets=my_onsets, downbeats=downbeats, tempo=tempo, time_signature=time_signature\n",
    "    )\n",
    "    # TEMPO SLOPE\n",
    "    ts = get_feature_data(\n",
    "        TempoSlope, ['tempo_slope', 'tempo_drift'],\n",
    "        my_beats=pd.concat([my_beats, their_beats], axis=1).mean(axis=1)\n",
    "    )\n",
    "    # TEMPO STABILITY\n",
    "    tstab = get_feature_data(\n",
    "        RollingIOISummaryStats, ['rolling_std_count_nonzero', 'rolling_std_median'],\n",
    "        my_onsets=my_beats, downbeats=downbeats, bar_period=4\n",
    "    )\n",
    "    # Return a single dictionary that combines the summary dictionary for all the features\n",
    "    return dict(**track.item, **bur, **pc, **pcb, **pcd, **pa, **ioi, **ts, **tstab, tempo=tempo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we extract features from all tracks in parallel (should take < 5 minutes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=-1, verbose=5) as parallel:\n",
    "    res = parallel(delayed(process_track)(t) for t in onsets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now convert all of these features to a dataframe, sort by the pianist in the recording, and print a few rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res).sort_values('pianist').reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot correlations between features\n",
    "Before we clean outliers, we should plot the correlations between the raw values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[PREDICTORS].isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HeatMapFeatureCorrelation(df[PREDICTORS]).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean dataset\n",
    "### Identify outliers\n",
    "When cleaning the dataset, we first need to identify outlying values in our features. We set outlying values to `np.nan` so that we can set them to the overall average later, rather than removing the track completely. To start, we clean `self_coupling`, `coupling_bass`, or `coupling_drums` when `nobs < 30`: this means that we have fewer than 10 terms for each of our 3 predictors in the regression."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean = df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in ['self_coupling', 'coupling_bass', 'coupling_drums']:\n",
    "    # If we have NaN obs, it's because the model failed to compile, so replace this with 0\n",
    "    clean[f'nobs'] = clean[f'nobs'].fillna(0)\n",
    "    clean.loc[clean['nobs'] < 30, col] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the same reason, we clean `coupling_bass_piano` (i.e. bass coupling to piano) when `nobs_bass < 30` (sim. for `coupling_drums_piano` and `nobs_drums`)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in ['bass', 'drums']:\n",
    "    # If we have NaN obs, it's because the model failed to compile, so replace this with 0\n",
    "    clean[f'nobs_{col}'] = clean[f'nobs_{col}'].fillna(0)\n",
    "    clean.loc[clean[f'nobs_{col}'] < 30, f'coupling_piano_{col}'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean `bur_log_mean` and `bur_log)std` when `-2 <= average_bur <= 2` (these are the values discussed by Corcoran and Frieler) OR `bur_log_count_nonzero < 15` (we have fewer than 15 BURs in the track)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in ['bur_log_mean', 'bur_log_std']:\n",
    "    clean.loc[clean['bur_log_mean'] < -2, col] = np.nan\n",
    "    clean.loc[clean['bur_log_mean'] > 2, col] = np.nan\n",
    "    clean.loc[clean['bur_log_count_nonzero'] < 15, col] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean `bass/drums_prop_async` when `piano_prop_async_count_nonzero < 15`, i.e. we have fewer than 15 beat asynchronies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in ['bass', 'drums']:\n",
    "    clean.loc[clean['prop_async_count_nonzero'] < 10, f'{col}_prop_async_nanmean'] = np.nan\n",
    "    clean.loc[clean['prop_async_count_nonzero'] < 10, f'{col}_prop_async_nanstd'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean `tempo_slope` and `tempo_drift` when `tempo_slope.abs() < 0.15`, i.e. we accelerated (or decelerated) by more than 0.15 beats-per-minute-per-second. This is the upper limit of values suggested to be realistic for jazz ensembles in the control condition of our previous experiment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean.loc[clean['tempo_slope'].abs() > 0.15, 'tempo_slope'] = np.nan\n",
    "clean.loc[clean['tempo_slope'].abs() > 0.15, 'tempo_drift'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean `rolling_std_median` when `rolling_std_median > 0.4`, i.e. the median standard deviation for a four-second window of 1/4 note beats is greater than 40 milliseconds. This is the upper limit of values for this variable from our earlier experiment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean.loc[clean['rolling_std_median'] > 0.4, 'rolling_std_median'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get number of missing values per predictor\n",
    "Let's see how many missing values we have per predictor variable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "COL_MAPPING = {\n",
    "    'bur_log_mean': 'Beat-upbeat ratio, mean',\n",
    "    'bur_log_std': 'Beat-upbeat ratio, std.',\n",
    "    'lz77_mean': 'Compression score, mean',\n",
    "    'lz77_std': 'Compression score, std',\n",
    "    'n_onsets_mean': 'Onset density, mean',\n",
    "    'n_onsets_std': 'Onset density, std',\n",
    "    'bass_prop_async_nanmean': 'Piano→Bass, async mean',\n",
    "    'bass_prop_async_nanstd': 'Piano→Bass, async std.',\n",
    "    'drums_prop_async_nanmean': 'Piano→Drums, async mean',\n",
    "    'drums_prop_async_nanstd': 'Piano→Drums, async std.',\n",
    "    'coupling_bass': 'Piano→Bass, coupling',\n",
    "    'coupling_piano_bass': 'Bass→Piano, coupling',\n",
    "    'coupling_drums': 'Piano→Drums, coupling',\n",
    "    'coupling_piano_drums': 'Drums→Piano, coupling',\n",
    "    'self_coupling': 'Piano→Piano, coupling',\n",
    "    'rolling_std_median': 'Tempo stability',\n",
    "    'tempo': 'Tempo average',\n",
    "    'tempo_slope': 'Tempo slope',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "miss = clean[PREDICTORS].isna().sum().reset_index()\n",
    "miss['cat'] = miss['index'].map(CATEGORY_MAPPING)\n",
    "print(miss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(miss.groupby('cat')[0].mean() / 300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CountPlotMissingValues(clean[PREDICTORS], CATEGORY_MAPPING).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Impute missing values based on feature average\n",
    "We can now set missing values to the average for that feature across the entire dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = clean[PREDICTORS].fillna(clean[PREDICTORS].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check that we've filled all missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X.isna().values.any())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encode categorical prediction variable\n",
    "Next, we need to encode our predictor variable (currently a list of names) to integers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can store the labels for later use\n",
    "pianist_labels = clean[PREDICTION].unique().tolist()\n",
    "y = clean[PREDICTION].map({val: i for i, val in enumerate(pianist_labels)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest modelling (new)\n",
    "### Create the model class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PREDICTORS_CATEGORIES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RandomForestFit:\n",
    "    \"\"\"Provides a wrapper around `StratifiedKFold` and `RandomForestClassifier`\"\"\"\n",
    "    col_idxs = [[0, 1], [2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17]]\n",
    "\n",
    "    def __init__(self, params: dict = None):\n",
    "        self.params = params if params is not None else {}\n",
    "        # I think we can probably shuffle the data first.\n",
    "        # The data is unordered (not time-series), so unlikely to have a negative effect\n",
    "        self.skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        self.predicts, self.importances, self.grouped_importances = [], [], []\n",
    "        self.trees = []\n",
    "\n",
    "    def fit_model(self, x_data, y_data):\n",
    "        fits = [self._fit_model(x_data, y_data, x_idx, y_idx) for x_idx, y_idx in self.skf.split(x_data, y_data)]\n",
    "        predicts, feature_importances, grouped_importance = list(zip(*fits))\n",
    "        self.predicts = self._format_predicts_df(predicts)\n",
    "        self.importances = self._format_importances_df(feature_importances)\n",
    "        self.grouped_importances = pd.concat(grouped_importance, axis=1)\n",
    "        return self\n",
    "\n",
    "    def get_global_accuracy(self) -> float:\n",
    "        \"\"\"Returns the percentage of correct predictions\"\"\"\n",
    "        return self.predicts['correct'].value_counts(normalize=True)[True]\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_importances_df(importances) -> pd.DataFrame:\n",
    "        importances_df_ = (\n",
    "            pd.concat(importances, axis=1)\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={'index': 'feature'})\n",
    "        )\n",
    "        importances_df_['category'] = importances_df_['feature'].map(CATEGORY_MAPPING)\n",
    "        return importances_df_\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_predicts_df(predicts) -> pd.DataFrame:\n",
    "        predict_df_ = (\n",
    "            pd.concat(predicts, axis=0)\n",
    "            .sort_index()\n",
    "            .rename(columns={0: 'prediction', 'pianist': 'actual'})\n",
    "        )\n",
    "        predict_df_['correct'] = predict_df_['prediction'] == predict_df_['actual']\n",
    "        for var in ['prediction', 'actual']:\n",
    "            predict_df_[var] = predict_df_[var].map({i: val for i, val in enumerate(pianist_labels)})\n",
    "        return predict_df_\n",
    "\n",
    "    def _fit_model(self, x_data, y_data, train_idx, test_idx):\n",
    "        # Get our splits for this fold\n",
    "        X_train, X_test = x_data.iloc[train_idx], x_data.iloc[test_idx]\n",
    "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "        # Create the random forest model\n",
    "        rf = RandomForestClassifier(random_state=SEED, **self.params)\n",
    "        # Fit the random forest to the training data\n",
    "        rf.fit(X_train, y_train)\n",
    "        self.trees.append(rf)\n",
    "        # Predict the test data\n",
    "        y_predict = rf.predict(X_test)\n",
    "        # Get prediction probabilities\n",
    "        y_probs = rf.predict_proba(X_test)\n",
    "        # Calculate importance of individual features\n",
    "        fpi = self.permutation_importance(rf, X_test, y_test)\n",
    "        # Calculate importance of feature groups\n",
    "        gpi = self.grouped_permutation_importance(rf, X_test, y_test)\n",
    "        # Return the prediction and feature importance dataframes\n",
    "        return (\n",
    "            self._format_predict_df(y_predict, y_probs, y_test),\n",
    "            pd.DataFrame(fpi.importances, index=X.columns),\n",
    "            pd.DataFrame(gpi.importances, index=PREDICTORS_CATEGORIES.keys())\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def permutation_importance(\n",
    "            estimator, X: pd.DataFrame, y: pd.DataFrame, n_repeats: int = 10,\n",
    "            scoring: str = 'accuracy', sample_weight: float = None, max_samples: float = 1.0\n",
    "    ) -> dict:\n",
    "        return permutation_importance(\n",
    "            estimator, X, y, n_repeats=n_repeats, random_state=SEED, n_jobs=N_JOBS,\n",
    "            sample_weight=sample_weight, max_samples=max_samples, scoring=scoring\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_predict_df(predict, predict_probs, actual):\n",
    "        y_probs = pd.DataFrame(predict_probs)\n",
    "        y_probs.columns = y_probs.columns.map({i: val for i, val in enumerate(pianist_labels)})\n",
    "        y_probs.index = actual.index\n",
    "        # Format the prediction series\n",
    "        y_predict_s = pd.Series(predict)\n",
    "        y_predict_s.index = actual.index\n",
    "        # Concatenate the predicted and actual pianist\n",
    "        return pd.concat([y_predict_s, actual, y_probs], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _grouped_permutation_score(\n",
    "        estimator, X, y, sample_weight, col_idxs,\n",
    "        random_state, n_repeats, scorer,\n",
    "    ):\n",
    "        random_state = check_random_state(random_state)\n",
    "        X_permuted = X.copy()\n",
    "        scores = []\n",
    "        shuffling_idx = np.arange(X_permuted.shape[0])\n",
    "        for _ in range(n_repeats):\n",
    "            random_state.shuffle(shuffling_idx)\n",
    "            col = X_permuted.iloc[shuffling_idx, col_idxs]\n",
    "            col.index = X_permuted.index\n",
    "            X_permuted[X_permuted.columns[col_idxs]] = col\n",
    "            scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n",
    "        return np.array(scores)\n",
    "\n",
    "    def grouped_permutation_importance(\n",
    "            self, estimator, X: pd.DataFrame, y: pd.DataFrame, random_state: float = SEED,\n",
    "            n_repeats: int = 10, scoring: str = 'accuracy', sample_weight: float = None\n",
    "    ) -> dict:\n",
    "        random_state = check_random_state(random_state)\n",
    "        random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n",
    "        scorer = check_scoring(estimator, scoring)\n",
    "        baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n",
    "        scores2 = [self._grouped_permutation_score(\n",
    "            estimator=estimator, X=X, y=y, col_idxs=col_idx, random_state=random_seed,\n",
    "            n_repeats=n_repeats, scorer=scorer, sample_weight=sample_weight,\n",
    "        ) for col_idx in self.col_idxs]\n",
    "        return _create_importances_bunch(baseline_score, np.array(scores2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get baseline accuracy\n",
    "Here, we just guess a random pianist for each track"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_random = accuracy_score(y, np.random.randint(y.min(), y.max(), len(y)))\n",
    "print(f\"Random guess accuracy: {accuracy_random}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you'd expect with 10 pianists and 30 tracks per pianist, we'll be correct approximately one in ten times if we just guess the pianist at random."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit the initial model and get accuracy\n",
    "Here we just use the default parameters for fitting the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_init = RandomForestFit().fit_model(X, y)\n",
    "print(f'Initial model accuracy: {rf_init.get_global_accuracy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter optimization\n",
    "Now, we can try and optimize the model using a randomized search over an array of parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# These are the parameters we'll sample from when optimizing\n",
    "test_params = dict(\n",
    "    # The number of trees to grow in the forest\n",
    "    n_estimators=[i for i in range(10, 1001, 1)],\n",
    "    # Max number of features considered for splitting a node\n",
    "    max_features=[None, 'sqrt', 'log2'],\n",
    "    # Max number of levels in each tree\n",
    "    max_depth=[None, *[i for i in range(1, 101, 1)]],\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split=[i for i in range(2, 11)],\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf=[i for i in range(1, 11)],\n",
    "    # Whether to sample data points with our without replacement\n",
    "    bootstrap=[True, False],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the `ParameterSampler` object for the required number of iterations\n",
    "sampler = ParameterSampler(test_params, n_iter=N_ITER, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_cached_tracks() -> list:\n",
    "    \"\"\"Tries to get cached optimised parameters, returns an empty list if cannot find\"\"\"\n",
    "    try:\n",
    "        return utils.load_csv(\n",
    "            fpath=rf\"{utils.get_project_root()}\\references\\parameter_optimisation\\corpus_chronology\",\n",
    "            fname='random_forest_opt'\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_forest(parameters: dict, iteration: int,):\n",
    "    \"\"\"Conducts one iteration of parameter optimisation, with given parameter\"\"\"\n",
    "    # Check our cache to see if we've already got results for this iteration\n",
    "    # As we use a set seed with `ParameterSampler`, we can get results with just the counter\n",
    "    cached_results = get_cached_tracks()\n",
    "    try:\n",
    "        cached_res = [o for o in cached_results if o['iteration'] == iteration][0]\n",
    "        # assert all(k in cached_res and parameters[k] == cached_res[k] for k in to_check)\n",
    "    except (IndexError, AssertionError):\n",
    "        pass\n",
    "    else:\n",
    "        return cached_res\n",
    "\n",
    "    # Create the forest model\n",
    "    forest = RandomForestFit(params=parameters)\n",
    "    # Fit the model to the data\n",
    "    forest.fit_model(X, y)\n",
    "    # Create the results dictionary and save\n",
    "    results_dict = {\n",
    "        'accuracy': forest.get_global_accuracy(),\n",
    "        'iteration': iteration,\n",
    "        **parameters\n",
    "    }\n",
    "    utils.save_csv(\n",
    "        results_dict,\n",
    "        rf\"{utils.get_project_root()}\\references\\parameter_optimisation\\corpus_chronology\",\n",
    "        'random_forest_opt'\n",
    "    )\n",
    "    # Return the fitted forest object\n",
    "    return results_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use lazy parallelization to create the forest and fit to the data\n",
    "with Parallel(n_jobs=N_JOBS, verbose=11) as parallel:\n",
    "    rfs_fitted = parallel(delayed(create_forest)(params, num) for num, params in enumerate(sampler))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the optimized parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe of parameters and accuracy scores\n",
    "opt_df = pd.DataFrame(rfs_fitted)\n",
    "opt_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the parameter combination that yielded the best accuracy\n",
    "best_params = opt_df[opt_df['accuracy'] == opt_df['accuracy'].max()][test_params.keys()].to_dict('records')[0]\n",
    "best_params['max_depth'] = int(best_params['max_depth'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the optimization process doesn't have to be re-run each time, the final parameter set is given below in plain text:\n",
    "\n",
    "`{\n",
    "    'n_estimators': 50,\n",
    "    'max_features': 'log2',\n",
    "    'max_depth': 30,\n",
    "    'bootstrap': True,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2\n",
    " }`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit the optimized model and get accuracy\n",
    "Now, we use the best combination of parameters from our optimization process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BEST_PARAMS = {'n_estimators': 50, 'max_features': 'log2', 'max_depth': 30, 'bootstrap': True, 'min_samples_leaf': 1, 'min_samples_split': 2}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_opt = RandomForestFit(params=BEST_PARAMS).fit_model(X, y)\n",
    "print(f'Optimized model accuracy: {rf_opt.get_global_accuracy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serialise model and save"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_all = RandomForestClassifier(random_state=SEED, **BEST_PARAMS)\n",
    "rf_all.fit(X, clean[PREDICTION])\n",
    "dump(rf_all, f'{utils.get_project_root()}/models/pianist_predictor.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot results\n",
    "### Plot feature importance\n",
    "Which rhythmic features are most important in defining the personal style of a particular pianist?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HeatMapPredictionProbDendro(rf_opt.predicts, include_images=False).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get accuracy scores\n",
    "Mean accuracy, top-k (`k=3`) accuracy, Cohen's kappa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score, accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "act = rf_opt.predicts['actual'].map({v: k for k, v in enumerate(rf_opt.predicts['actual'].unique())}).values\n",
    "pred = rf_opt.predicts['prediction'].map({v: k for k, v in enumerate(rf_opt.predicts['actual'].unique())}).values\n",
    "probs = rf_opt.predicts[pianist_labels].values\n",
    "print('Mean accuracy:', accuracy_score(act, pred))\n",
    "print('Top-k accuracy:', top_k_accuracy_score(act, probs, k=3))\n",
    "print('Cohen kappa:', cohen_kappa_score(act, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(act, pred, normalize='true'))\n",
    "cm.index = pianist_labels\n",
    "cm.columns = pianist_labels\n",
    "cm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get feature importances\n",
    "Average importance per feature, per category"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imp_feat = (\n",
    "    rf_opt.importances.copy(deep=True)\n",
    "    .melt(id_vars=['feature', 'category'])\n",
    "    .groupby('feature')\n",
    "    ['value']\n",
    "    .mean()\n",
    "    .sort_values()\n",
    ")\n",
    "print(imp_feat * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imp_cat = (\n",
    "    rf_opt.grouped_importances.copy(deep=True)\n",
    "    .mean(axis=1)\n",
    "    .sort_values(ascending=True)\n",
    ")\n",
    "print(imp_cat * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting first and last recordings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = clean[PREDICTORS].fillna(clean[PREDICTORS].mean())\n",
    "# We can store the labels for later use\n",
    "pianist_labels = clean[PREDICTION].unique().tolist()\n",
    "y = clean[PREDICTION].map({val: i for i, val in enumerate(pianist_labels)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean['recording_year_'] = clean['recording_year'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_idxs = (\n",
    "    clean.sort_values(by=['pianist', 'recording_year_', 'mbz_id'], ascending=True)\n",
    "    .groupby('pianist')\n",
    "    .head(6)\n",
    "    .index\n",
    "    .sort_values()\n",
    "    .to_numpy()\n",
    ")\n",
    "max_idxs = (\n",
    "    clean.sort_values(by=['pianist', 'recording_year_', 'mbz_id'], ascending=False)\n",
    "    .groupby('pianist')\n",
    "    .head(6)\n",
    "    .index\n",
    "    .sort_values()\n",
    "    .to_numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def minmax_acc(test_idx) -> float:\n",
    "    train_idx = np.sort(np.array([*(set(X.index) - set(test_idx))]))\n",
    "    assert not set(test_idx).issubset(set(train_idx))\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    # Create the random forest model\n",
    "    rf_fl = RandomForestClassifier(random_state=SEED, **BEST_PARAMS)\n",
    "    # Fit the random forest to the training data\n",
    "    rf_fl.fit(X_train, y_train)\n",
    "    fl_pred = rf_fl.predict(X_test)\n",
    "    # Get accuracy for predicting the set\n",
    "    return accuracy_score(y_test, fl_pred)\n",
    "\n",
    "min_acc = minmax_acc(min_idxs)\n",
    "max_acc = minmax_acc(max_idxs)\n",
    "print(min_acc, max_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def permute_model(rand_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=rand_state)\n",
    "    # Create the random forest model\n",
    "    rf_rand = RandomForestClassifier(random_state=SEED, **BEST_PARAMS)\n",
    "    # Fit the random forest to the training data\n",
    "    rf_rand.fit(X_train, y_train)\n",
    "    rand_pred = rf_rand.predict(X_test)\n",
    "    return accuracy_score(y_test, rand_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=-1, verbose=5) as parallel:\n",
    "    accs = parallel(delayed(permute_model)(state) for state in range(vutils.N_BOOT))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.std(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HistPlotFirstLastP(accs, min_acc, max_acc).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regressions of age vs strongest predictors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean['recording_year_'] = clean['recording_year'].astype(int)\n",
    "clean['career_progress'] = clean.groupby('pianist')['recording_year_'].transform(lambda g: g - g.min())\n",
    "clean['career_progress'] /= clean['career_progress'].max()\n",
    "clean['jazz_progress'] = clean['recording_year_'] - clean['recording_year_'].min()\n",
    "clean['jazz_progress'] /= clean['jazz_progress'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "predictors = ['drums_prop_async_nanmean', 'tempo_slope', 'bur_log_mean', 'n_onsets_mean', 'coupling_piano_drums']\n",
    "conr = []\n",
    "marr = []\n",
    "for predict in predictors:\n",
    "    vars_ = [predict, 'pianist', 'career_progress', 'jazz_progress']\n",
    "    data = clean.copy(deep=True)\n",
    "    data[predict] = data[predict].fillna(data[predict].mean())\n",
    "    md = smf.mixedlm(f'{predict}~career_progress', groups=data['pianist'], data=data, re_formula='~career_progress').fit(method=['lbfgs'])\n",
    "    print(md.summary())\n",
    "    # Variance explained by the fixed effects: we need to use md.predict() with the underlying data to get this\n",
    "    var_fixed = md.predict().var()\n",
    "    # Variance explained by the random effects\n",
    "    var_random = float(md.cov_re.to_numpy().mean())\n",
    "    # Variance of the residuals\n",
    "    var_resid = md.scale\n",
    "    # Total variance of the model\n",
    "    total_var = var_fixed + var_random + var_resid\n",
    "    # Calculate the r2 values and append to the model\n",
    "    conr.append((var_fixed + var_random) / total_var)\n",
    "    marr.append(var_fixed / total_var)\n",
    "print('mean conditional r2', np.mean(conr) * 100, 'stdev', np.std(conr) * 100)\n",
    "print('mean marginal r2', np.mean(marr) * 100, 'stdev', np.std(marr) * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean.groupby('pianist')['recording_year_'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "class RegPlotCareerJazzProgress(vutils.BasePlot):\n",
    "    predictors = ['drums_prop_async_nanmean', 'tempo_slope', 'bur_log_mean', 'n_onsets_mean', 'coupling_piano_drums']\n",
    "    palette = sns.color_palette('tab10')\n",
    "    palette = [palette[2], palette[4], palette[0], palette[1],   palette[3]]\n",
    "    markers = ['o', 's', 'D', '^', 'p']\n",
    "    categories = ['Feel', 'Tempo', 'Swing', 'Complexity', 'Interaction']\n",
    "    REG_KWS = dict(color=vutils.BLACK, linewidth=vutils.LINEWIDTH * 2, ls=vutils.LINESTYLE)\n",
    "    xspace = np.linspace(0, 1, 100)\n",
    "    N_JOBS = -1\n",
    "\n",
    "    def __init__(self, model_df, cat_mapping, **kwargs):\n",
    "        self.corpus_title = 'corpus_chronology'\n",
    "        self.cat_mapping = cat_mapping\n",
    "        self.df = model_df.copy(deep=True)\n",
    "        super().__init__(\n",
    "            figure_title=fr'random_forest_plots\\regplot_careerjazzprogress_{self.corpus_title}', **kwargs\n",
    "        )\n",
    "        self.fig, self.ax = plt.subplots(\n",
    "            nrows=2, ncols=3, figsize=(vutils.WIDTH, vutils.WIDTH / 2), sharex=True, sharey=False\n",
    "        )\n",
    "        self.ax.flatten()[-1].axis('off')\n",
    "\n",
    "    def get_line(self, model, mean):\n",
    "        params = model.params\n",
    "        intercept = params['Intercept']\n",
    "        jp = params['jazz_progress'] * mean\n",
    "        return [intercept + jp + (i * params['career_progress']) for i in self.xspace]\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_model(data_, predict_):\n",
    "        return (\n",
    "            smf.mixedlm(\n",
    "                f'{predict_}~career_progress+jazz_progress',\n",
    "                groups=data_['pianist'],\n",
    "                data=data_,\n",
    "                re_formula='~jazz_progress'\n",
    "            ).fit(method=['lbfgs'])\n",
    "        )\n",
    "\n",
    "    def bootstrap(self, data_, predict_) -> pd.DataFrame:\n",
    "        def shuffle_data_and_fit(state):\n",
    "            samp = data_.sample(frac=1, replace=True, random_state=state)\n",
    "            mode = self.fit_model(samp, predict_)\n",
    "            return pd.Series(self.get_line(mode, np.mean(samp['jazz_progress'])))\n",
    "\n",
    "        with Parallel(n_jobs=self.N_JOBS, verbose=3) as par:\n",
    "            boots = par(delayed(shuffle_data_and_fit)(num) for num in range(vutils.N_BOOT))\n",
    "        boot_ys = pd.concat(boots, axis=1)\n",
    "        low = boot_ys.apply(lambda r: np.percentile(r, 2.5), axis=1)\n",
    "        high = boot_ys.apply(lambda r: np.percentile(r, 97.5), axis=1)\n",
    "        return pd.DataFrame(dict(x=self.xspace, low=low, high=high))\n",
    "\n",
    "    def _create_plot(self):\n",
    "        for ax, predict, col, mark in zip(self.ax.flatten(), self.predictors, self.palette, self.markers):\n",
    "            data = self.df.copy(deep=True)\n",
    "            data[predict] = data[predict].fillna(data[predict].mean())\n",
    "            md = self.fit_model(data, predict)\n",
    "            sns.scatterplot(\n",
    "                data=data, x='career_progress', y=predict, label=self.cat_mapping[predict],\n",
    "                ax=ax, color=col, marker=mark, s=100, alpha=vutils.ALPHA\n",
    "            )\n",
    "            results_y = pd.DataFrame(self.get_line(md, np.mean(data['jazz_progress'])))\n",
    "            ax.plot(self.xspace, results_y, **self.REG_KWS)\n",
    "            boot_df = self.bootstrap(data, predict)\n",
    "            ax.fill_between(boot_df['x'], boot_df['low'], boot_df['high'], color=vutils.BLACK, alpha=vutils.ALPHA)\n",
    "\n",
    "    def _format_ax(self):\n",
    "        leg, hand = [], []\n",
    "        for a, tit, col in zip(self.ax.flatten(), self.categories, self.palette):\n",
    "            l, h = a.get_legend_handles_labels()\n",
    "            leg.extend(l)\n",
    "            hand.extend(h)\n",
    "            a.get_legend().remove()\n",
    "            a.grid(axis='x', which='major', **vutils.GRID_KWS)\n",
    "            a.set_title(tit, color=col)\n",
    "            a.set_ylabel(COL_MAPPING[a.get_ylabel()], color=col)\n",
    "            a.set(xlabel='', xlim=(-0.05, 1.05))\n",
    "            plt.setp(a.spines.values(), linewidth=vutils.LINEWIDTH)\n",
    "            a.tick_params(axis='both', bottom=True, width=vutils.TICKWIDTH)\n",
    "        self.fig.legend(\n",
    "            leg, hand, loc='lower right', title='Category', frameon=True,\n",
    "            framealpha=1, edgecolor=vutils.BLACK, bbox_to_anchor=(0.9, 0.15)\n",
    "        )\n",
    "\n",
    "    def _format_fig(self):\n",
    "        self.fig.supxlabel('Career progress')\n",
    "        self.fig.subplots_adjust(left=0.07, right=0.99, bottom=0.1, top=0.95, wspace=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RegPlotCareerJazzProgress(clean, CATEGORY_MAPPING).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary logistic regression of two clusters -- impressionistic vs blues"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "impression = ['Bill Evans', 'Bud Powell', 'John Hicks', 'Keith Jarrett', 'Tommy Flanagan']\n",
    "bop = ['Ahmad Jamal', 'Junior Mance', 'Kenny Barron', 'McCoy Tyner', 'Oscar Peterson']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean.loc[clean['pianist'].isin(impression), 'cluster'] = 0\n",
    "clean.loc[clean['pianist'].isin(bop), 'cluster'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = clean[reversed([\n",
    "    'bass_prop_async_nanmean',\n",
    "    'bass_prop_async_nanstd',\n",
    "    'drums_prop_async_nanmean',\n",
    "    'drums_prop_async_nanstd',\n",
    "    'tempo',\n",
    "    'tempo_slope',\n",
    "    'rolling_std_median',\n",
    "    'bur_log_mean',\n",
    "    'bur_log_std',\n",
    "    'n_onsets_mean',\n",
    "    'n_onsets_std',\n",
    "    'lz77_mean',\n",
    "    'lz77_std',\n",
    "    'self_coupling',\n",
    "    'coupling_bass',\n",
    "    'coupling_drums',\n",
    "    'coupling_piano_bass',\n",
    "    'coupling_piano_drums'\n",
    "])]\n",
    "X = X.fillna(X.mean()).apply(stats.zscore)\n",
    "X = sm.add_constant(X)\n",
    "y = clean['cluster'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md = sm.Logit(y, X).fit()\n",
    "print(md.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = md.predict(X)\n",
    "print(accuracy_score(y, y_pred.round().astype(int)))\n",
    "print(roc_auc_score(y, y_pred))\n",
    "print(roc_auc_score(y, np.random.choice([0, 1], 300)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RocPlotLogRegression(y, y_pred).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = pd.concat([md.params.rename('coeff'), md.conf_int().rename(columns={0: 'low', 1: 'high'})], axis=1).apply(np.exp).reset_index(drop=False)[1:]\n",
    "# params['low'] = params['coeff'] - params['low']\n",
    "# params['high'] -= params['coeff']\n",
    "params['category'] = params['index'].map(CATEGORY_MAPPING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "StripPlotLogitCoeffs(md, category_mapping=CATEGORY_MAPPING).create_plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}