# Working with `OnsetMaker` classes
(working-with-onsetmaker)=

Once you've loaded the corpus (either by calling `src.utils.unserialise_object` on a `.p` file, or pointing `src.utils.load_corpus_from_files` on the directory containing downloaded `.csv` and `.p` files), you'll end up with a list of `src.detect.detect_utils.OnsetMaker` classes. Each of these classes contains onset and beat data extracted from a single track. 

You can work with this list as with any iterable in Python:

```
# Process individual tracks
for track in res:
    ...
# Get just the first track
first_track = res[0] 
```

In the sections below, we explain the most important attributes of this class.

## Metadata (`OnsetMaker.item`)

Metadata relating to each track is contained inside the `OnsetMaker.item` attribute (`dict`). This information is taken from the corpus files, located in `.\references\{filename}.xlsx`. For more information as to what each key-value pair refers to here, see {ref}`the page on creating a new corpus file <new-corpus>`.

## Onsets and beats (`OnsetMaker.ons`)

The individual onsets for each instrumental part can be accessed undet the `OnsetMaker.ons` attribute. This is a Python dictionary, with each key (`str`) describing each individual array (`np.ndarray`) that is contained as a value. The following table summarises these key-value pairs:

| Class              | Description                                                                                               |
|--------------------|-----------------------------------------------------------------------------------------------------------|
| `mix`              | Timestamps in seconds for detected quarter note beats in the audio mixture                                |
| `metre_auto`       | Beat numbers for each bar in the audio mixture, automatically generated by beat tracking algorithm        |
| `downbeats_auto`   | Timestamps for automatically generated downbeats (i.e., `mix[np.where(metre_auto == 1)]`)                 |
| `metre_manual`     | Beat numbers for each bar in the audio mixture, generated by extrapolating `track.item['first_downbeat']` |
| `downbeats_manual` | Timestamps for manually generated downbeats (i.e., `mix[np.where(metre_manual == 1)]`)                    |
| `piano`            | Timestamps for each onset detected in the source-separated piano audio                                    |
| `bass`             | Timestamps for each onset detected in the source-separated bass audio                                     |
| `drums`            | Timestamps for each onset detected in the source-separated drums audio                                    |

## Matched onsets and beats (`OnsetMaker.summary_dict`)

As with `OnsetMaker.ons`, the `OnsetMaker.summary_dict` attribute is a dictionary of key-value pairs where keys are `str` descriptors and values are `np.ndarray` types. Unlike `OnsetMaker.ons`, however, the only onsets in `OnsetMaker.summary_dict` are those that are matched to each quarter note beat, by running `OnsetMaker.match_onsets_and_beats()`.

The lengths of all `np.ndarray` objects in `OnsetMaker.summary_dict.values()` should be the same for a single `OnsetMaker` instance. In cases where a beat could not be matched with an onset for a particular instrument, a missing value (`np.nan`) is used for that beat. This means that `OnsetMaker.summary_dict` can be passed directly to `pd.DataFrame`, to easily manipulate the data in `pandas`:

```
first_track_df = pd.DataFrame(first_track.summary_dict)
first_track_df.head(3)

>>>    beats     piano      bass     drums  metre_auto  metre_manual
>>> 0   0.17  0.177052  0.174150  0.182857         1.0           1.0
>>> 1   0.55  0.565986  0.571791  0.568889         2.0           2.0
>>> 2   0.92  0.943311       NaN  0.983946         3.0           3.0

# To access just the matched beats for the drummer:
first_track_df['drums'].head(3)

>>> 0    0.182857
>>> 1    0.568889
>>> 2    0.983946
```

## Extra data (only relevant if building from source)

If you've built the database from source, there are a few additional attributes under each `OnsetMaker` class instances created during the onset detection process: see below for a description

:::{dropdown} Additional attributes

| Field                    | Description                                                                                         |
|--------------------------|-----------------------------------------------------------------------------------------------------|
| `.silent_perc`           | The percentage of each audio file deemed to be `silent` (see `OnsetMaker.get_silent_track_percent`) |
| `.snr`                   | The signal-to-noise ratio for each audio file                                                       |
| `.spectral_flatness`     | The spectral flatness for each audio file                                                           |

:::